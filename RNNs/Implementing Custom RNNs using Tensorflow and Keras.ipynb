{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c7cbfa",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Spinkk/TeachingTensorflow/blob/main/RNNs/Implementing%20Custom%20RNNs%20using%20Tensorflow%20and%20Keras.ipynb)\n",
    "\n",
    "# Implementing Custom RNNs using Tensorflow and Keras\n",
    "\n",
    "In this notebook, I show you how a basic RNN that uses a simple recurrent cell (no LSTM or GRU) transforms input by applying the same matrix multiplications at each time step, using the latest output as one of the inputs. After having investigated the computations involved in recurrent neural networks, you will also see how you can write your own RNN wrapper and simple rnn cell layers in tensorflow, which you can use as an orientation for the homework.\n",
    "\n",
    "\n",
    "Remember that tf.keras.layers.Dense layers perform a matrix multiplication on their input, so we use these layers to define our RNN cell.\n",
    "\n",
    "A basic RNN takes its own current hidden state (which is a vector of dimension hidden_dim for each example in the batch) and linearly transforms it with a weight matrix, creating a vector of n_outputs dimensions. In addition (quite literally) to that, the RNN cell takes the input at the current time-step t (which is the feature vector in the example for time-step t) and transforms it with another matrix multiplication to the same dimensionality of the hidden state. Adding the results from the two matrix multiplications together with a bias (and applying a tanh activation to the sum), we obtain the hidden state of the RNN-cell that will then be used at the next time-step t+1, together with the input data feature vector at t+1.\n",
    "\n",
    "More formally written, the computation of a simple RNN cell at one time-step can be described as follows:\n",
    "\n",
    "### $$h^{<t+1>} = tanh ( W_{hh} h^{<t>} + W_{xh} x^{<t>} + b )$$ \n",
    "\n",
    "Where $x^{<t>}$ is the feature input at time-step t, and $W_{xh}$ is a matrix of shape x (the dimensionality of the input feature vector at a single time-step) by h (the dimensionality of the hidden state).\n",
    "\n",
    "\n",
    "Below we implement this type of an RNN on some input (not inside a tf.keras.Model or tf.keras.Layer but as separate layers since we don't do any model fitting here). First we show how to use tensorflow/keras inbuilt RNN cell and RNN wrapper layers to achieve this. By the end of the notebook you'll know how to implement these layers yourself from scratch.\n",
    "\n",
    "Note: You will see the context defined by \"with tf.device('/device:cpu:0'):\" quite a lot in this notebook. This is to force tensorflow to do computations on the device specified. Since we do not need a GPU for this notebook, it's better to not have tensorflow reserve the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70ee7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f192659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the RNN, which is the last hidden state is \n",
      "[[-0.55842763  0.7973356   0.19838   ]\n",
      " [ 0.5054061   0.58539706 -0.84264874]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    \n",
    "    # a batch with 6 time steps of 16 features. The time-dimension is the second after the batch dimension.\n",
    "    input_shape = (2, 6, 16)\n",
    "\n",
    "    # The length of the resulting vector (similar to the units argument in Dense layers)\n",
    "    hidden_dim = 3\n",
    "    \n",
    "    input_sequence = tf.random.uniform(shape = input_shape)\n",
    "    \n",
    "    simple_RNN_cell = tf.keras.layers.SimpleRNNCell(hidden_dim)\n",
    "    \n",
    "    # return_sequences=False means we only output the final hidden_state.\n",
    "    RNN = tf.keras.layers.RNN(simple_RNN_cell, return_sequences=False)\n",
    "    \n",
    "    output = RNN(input_sequence)\n",
    "    \n",
    "    print(f\"The output of the RNN, which is the last hidden state is \\n{output}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e4d2c",
   "metadata": {},
   "source": [
    "# tf.keras.layers.RNN and tf.keras.layers.SimpleRNNCell from scratch\n",
    "\n",
    "Now we want to look at how we can implement what happens inside the two pre-defined layers that we have used above.\n",
    "\n",
    "To do so, we create two dense layers carrying the matrices $W_{hh}$ and $W_{xh}$ that are involved in the simple RNN cell. For the bias $b$ we create a separate tf.variable. Notice that we disable the use of a bias in the dense layers and do not use an activation either, because we want a simple matrix multiplication.\n",
    "\n",
    "With the weight matrices and the bias defined, we can start to iterate over the same input sequence data that we have used before. The initial hidden state of the RNN cell is set to be a vector of zeros before looping over the input sequence.\n",
    "\n",
    "At each time-step we use the previous hidden-state of the cell and update it, following the equation shown in the beginning of the notebook. The output of this custom RNN is then just the final hidden-state vector.\n",
    "\n",
    "Notice how we use the same two weight matrices and the same bias for different temporal parts of the input data. This is why RNNs can be thought of as another variant of weight sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b57de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[-0.03773887  0.18126507 -0.47590804]\n",
      " [-0.7169903  -0.26814744  0.1255381 ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    ### We've seen the output of the randomly initialized simple RNN.\n",
    "    ### Now we want to show the computations that were involved by doing the same with dense layers \n",
    "    \n",
    "    ### Remember that dense layers without activation are just matrix multiplication of a weight matrix with input\n",
    "    \n",
    "    # We create the Dense layers (that will store the matrices)\n",
    "    dense_layer_hstate = tf.keras.layers.Dense(hidden_dim, activation=None, use_bias=False)\n",
    "    dense_layer_input = tf.keras.layers.Dense(hidden_dim, activation=None, use_bias=False)\n",
    "    bias = tf.Variable(tf.zeros(hidden_dim, dtype=tf.float32), name=\"biases\")\n",
    "    \n",
    "    # create the initial hidden state\n",
    "    state = tf.zeros((1,hidden_dim), tf.float32)\n",
    "    \n",
    "    # iterate over the time-steps\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        # on this iteration we use the input at timestep t\n",
    "        input_t =input_sequence[:,t,:]\n",
    "        \n",
    "        # we compute the sum of the input at t matrix multiplied, with the previous state matrix multiplied\n",
    "        # and an additional bias added.\n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "        \n",
    "        # finally we use hyperbolic tangent as an activation function and update the RNN cell state\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{state}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d88524",
   "metadata": {},
   "source": [
    "# Copying the weights of the pre-defined RNN to our custom RNN to verify the implementation\n",
    "\n",
    "To make sure that we do the same computations as in the pre-defined tf.keras.layers.SimpleRNNCell and the wrapper layer (which implements the for loop) tf.keras.layers.RNN, we take the weights that we have used before and assign them to the two dense layers and the bias. We observe that we now get the exact same output - our implementation is correct.\n",
    "\n",
    "This type of testing can be helpful to verify your custom implementations of the computations involved in the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cee053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the same weights as the pre-defined RNN, our custom RNN's output is\n",
      " [[-0.55842763  0.7973356   0.19838   ]\n",
      " [ 0.5054061   0.58539706 -0.84264874]]\n",
      "\n",
      "The outputs of the pre-defined RNN and our custom RNN are the same: True\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    # Now we want to copy the weights of this RNN to a fully connected model to obtain the same output\n",
    "    \n",
    "    RNN_cell_input_weights = RNN.trainable_variables[0]\n",
    "\n",
    "    RNN_cell_hstate_weights = RNN.trainable_variables[1]\n",
    "    \n",
    "    RNN_cell_biases = RNN.trainable_variables[2]\n",
    "    \n",
    "    dense_layer_hstate.weights[0].assign(tf.reshape(RNN_cell_hstate_weights, dense_layer_hstate.weights[0].shape))\n",
    "    dense_layer_input.weights[0].assign(tf.reshape(RNN_cell_input_weights, dense_layer_input.weights[0].shape))\n",
    "    bias.assign(RNN_cell_biases)\n",
    "    \n",
    "    # again we run our custom RNN, this time with the same weights as the tf.keras.layers.RNN version\n",
    "    \n",
    "    # same code as above\n",
    "    state = tf.zeros((1,hidden_dim), tf.float32)\n",
    "\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        input_t =input_sequence[:,t,:]\n",
    "        \n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "    \n",
    "    print(f\"With the same weights as the pre-defined RNN, our custom RNN's output is\\n {state}\\n\")\n",
    "    \n",
    "    print(f\"The outputs of the pre-defined RNN and our custom RNN are the same: {tf.reduce_all(state==output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7910b2",
   "metadata": {},
   "source": [
    "# Return sequences of hidden-states with an RNN\n",
    "\n",
    "Before we have set the return_sequences argument to False, and we ended up with a single vector as the output (the final hidden state) of the RNN. To obtain a sequence from the RNN, we want to return the hidden states from all the time-steps. This is useful for cases in which we want to translate from one sequence to another, like in language translation or audio-processing. To implement this with the pre-defined layers, we use the same simple rnn cell layer as before but re-instantiate the RNN wrapper layer with the new argument set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b005af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the RNN, which is the last hidden state is \n",
      " [[[ 0.78782976  0.6367441  -0.16496447]\n",
      "  [-0.14419898  0.57446164 -0.48047882]\n",
      "  [-0.40607536  0.6472315  -0.6515597 ]\n",
      "  [-0.23524919  0.9488684  -0.6779934 ]\n",
      "  [ 0.47880217  0.7325374  -0.7649211 ]\n",
      "  [-0.55842763  0.7973356   0.19838   ]]\n",
      "\n",
      " [[ 0.6847383   0.7199698  -0.54862726]\n",
      "  [ 0.35626864  0.9001869   0.00961799]\n",
      "  [-0.05822116  0.65383255 -0.07338887]\n",
      "  [ 0.01840258  0.2500062  -0.26280335]\n",
      "  [ 0.6293602   0.5622283  -0.6971662 ]\n",
      "  [ 0.5054061   0.58539706 -0.84264874]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    # simple_RNN_cell and input_sequence remain unchanged\n",
    "    \n",
    "    # return_sequences=True means we output the hidden_states of all time-steps.\n",
    "    RNN = tf.keras.layers.RNN(simple_RNN_cell, return_sequences=True)\n",
    "    \n",
    "    RNN_outputs = RNN(input_sequence)\n",
    "    \n",
    "    print(f\"The output of the RNN, which is the last hidden state is \\n {RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ed762",
   "metadata": {},
   "source": [
    "# Return sequences from scratch\n",
    "\n",
    "To implement this in a way that will allow you to also use this in tensorflow models that use the @tf.function decorator, we can't just create a list of the hidden states and append to it - tensorflow does not allow list appends in graph mode. Instead we can make use of a tf.TensorArray object (this also allows for the case in which we do not know how many time-steps we will process and thus can't know beforehand how many hidden-states we want to output).\n",
    "\n",
    "A tf.TensorArray object is not just a tensor and thus we need to call the .stack() method on it to obtain the hidden_states that it stores. Since the result has the batch-dimension and the time-dimension switched, we make use of a permuted transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c416bf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[[ 0.78782976  0.6367441  -0.16496447]\n",
      "  [-0.14419898  0.57446164 -0.48047882]\n",
      "  [-0.40607536  0.6472315  -0.6515597 ]\n",
      "  [-0.23524919  0.9488684  -0.6779934 ]\n",
      "  [ 0.47880217  0.7325374  -0.7649211 ]\n",
      "  [-0.55842763  0.7973356   0.19838   ]]\n",
      "\n",
      " [[ 0.6847383   0.7199698  -0.54862726]\n",
      "  [ 0.35626864  0.9001869   0.00961799]\n",
      "  [-0.05822116  0.65383255 -0.07338887]\n",
      "  [ 0.01840258  0.2500062  -0.26280335]\n",
      "  [ 0.6293602   0.5622283  -0.6971662 ]\n",
      "  [ 0.5054061   0.58539706 -0.84264874]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "\n",
    "    state = tf.zeros((input_sequence.shape[0], hidden_dim), tf.float32)\n",
    "\n",
    "    # initialize the hidden_states TensorArray that we want to output (shape is: batch, time-steps, h_dim)\n",
    "    hidden_states = tf.TensorArray(dtype=tf.float32, size = input_sequence.shape[1])\n",
    "\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        input_t =input_sequence[:,t,:]\n",
    "\n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "\n",
    "        # write the states to the TensorArray\n",
    "        hidden_states = hidden_states.write(t, state)\n",
    "\n",
    "    # transpose the sequence of hidden_states from TensorArray accordingly (batch and time dimensions switched)\n",
    "    custom_RNN_outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
    "\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{custom_RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186ca1",
   "metadata": {},
   "source": [
    "Now let us verify that the @tf.function decorator works with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb25148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[[ 0.78782976  0.6367441  -0.16496447]\n",
      "  [-0.14419898  0.57446164 -0.48047882]\n",
      "  [-0.40607536  0.6472315  -0.6515597 ]\n",
      "  [-0.23524919  0.9488684  -0.6779934 ]\n",
      "  [ 0.47880217  0.7325374  -0.7649211 ]\n",
      "  [-0.55842763  0.7973356   0.19838   ]]\n",
      "\n",
      " [[ 0.6847383   0.7199698  -0.54862726]\n",
      "  [ 0.35626864  0.9001869   0.00961799]\n",
      "  [-0.05822116  0.65383255 -0.07338887]\n",
      "  [ 0.01840258  0.2500062  -0.26280335]\n",
      "  [ 0.6293602   0.5622283  -0.6971662 ]\n",
      "  [ 0.5054061   0.58539706 -0.84264874]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    @tf.function\n",
    "    def tf_func():\n",
    "\n",
    "        state = tf.zeros((input_sequence.shape[0], hidden_dim), tf.float32)\n",
    "\n",
    "        # initialize the hidden_states TensorArray that we want to output (shape is: batch, time-steps, h_dim)\n",
    "        hidden_states = tf.TensorArray(dtype=tf.float32, size=input_sequence.shape[1])\n",
    "\n",
    "        for t in tf.range(input_shape[1]):\n",
    "            input_t =input_sequence[:,t,:]\n",
    "\n",
    "            x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "            state = tf.nn.tanh(x_sum)\n",
    "\n",
    "            # write the states to the TensorArray\n",
    "            hidden_states = hidden_states.write(t, state)\n",
    "\n",
    "        # transpose the sequence of hidden_states from TensorArray accordingly (batch and time dimensions switched)\n",
    "        custom_RNN_outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
    "        \n",
    "        return custom_RNN_outputs\n",
    "    custom_RNN_outputs = tf_func()\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{custom_RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f87760",
   "metadata": {},
   "source": [
    "Finally, we again verify that the outputs with return_sequences=True match between our custom implementation of the computations involved and the pre-defined RNN-wrapper and cell. Since we did not re-initialize the weights, the outputs should still be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc99e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_all(custom_RNN_outputs == RNN_outputs).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c2cb5",
   "metadata": {},
   "source": [
    "# Implementing custom RNNs with subclassing\n",
    "\n",
    "Let's start with writing the RNNWrapper layer which loops over the observations at the different time-steps.\n",
    "\n",
    "Like in tf.keras.layers.RNN, we implement a return_sequences argument that controls whether to return a sequence of hidden states or just the last hidden state.\n",
    "\n",
    "(!) Important: This RNNWrapper is written only with the Simple RNN Cell in mind. It does not (yet - it is part of your task for the homework to adjust this) allow for LSTM Cells because here we only take care of the hidden_states but not of the cell state (since it does not exist for the basic rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025a4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self, RNN_Cell, return_sequences=False):\n",
    "        super(RNNWrapper, self).__init__()\n",
    "        \n",
    "        self.return_sequences = return_sequences\n",
    "        \n",
    "        self.cell = RNN_Cell\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        \n",
    "        length = data.shape[1]\n",
    "        \n",
    "        # initialize state of the simple rnn cell\n",
    "        state = tf.zeros((data.shape[0], self.cell.units), tf.float32)\n",
    "        \n",
    "        # initialize array for hidden states (only relevant if self.return_sequences == True)\n",
    "        hidden_states = tf.TensorArray(dtype=tf.float32, size=length)\n",
    "\n",
    "        for t in tf.range(length):\n",
    "            input_t = data[:,t,:]\n",
    "\n",
    "            state = self.cell(input_t, state, training)\n",
    "\n",
    "            if self.return_sequences:\n",
    "                # write the states to the TensorArray\n",
    "                #hidden_states = hidden_states.write(t, state)\n",
    "                hidden_states.append(state)\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            # transpose the sequence of hidden_states from TensorArray accordingly \n",
    "            #(batch and time dimensions are otherwise switched after .stack())\n",
    "            outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
    "        \n",
    "        else:\n",
    "            # take the last hidden state of the simple rnn cell\n",
    "            outputs = state\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc983b29",
   "metadata": {},
   "source": [
    "Next we implement the SimpleRNNCell which is compatible with our custom RNN wrapper layer, because it only has a single hidden state (no cell state like the LSTM).\n",
    "\n",
    "Here we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41282a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSimpleRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_regularizer=None ):\n",
    "        super(CustomSimpleRNNCell, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        \n",
    "        self.dense_hstate = tf.keras.layers.Dense(units, \n",
    "                                                  kernel_regularizer=kernel_regularizer, \n",
    "                                                  use_bias=False)\n",
    "        \n",
    "        self.dense_input = tf.keras.layers.Dense(units, \n",
    "                                                 kernel_regularizer=kernel_regularizer, \n",
    "                                                 use_bias=False)\n",
    "        \n",
    "        self.bias = tf.Variable(tf.zeros(units), name=\"RNN_Cell_biases\")\n",
    "        \n",
    "        self.state_size = units\n",
    "        \n",
    "        \n",
    "    def call(self, input_t, state,training=False):\n",
    "        \n",
    "        # we compute the sum of the input at t matrix multiplied and the previous state matrix multiplied\n",
    "        # and an additional bias added.\n",
    "        x_sum = self.dense_input(input_t) + self.dense_hstate(state) + self.bias\n",
    "        \n",
    "        # finally we use hyperbolic tangent as an activation function to update the RNN cell state\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "        \n",
    "        return (state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ce580",
   "metadata": {},
   "source": [
    "Next, we define an RNN_model class, which uses the simple RNN to obtain a single feature vector from the time-series and makes a prediction based on this feature vector, using two fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6318bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(RNN_Model, self).__init__()\n",
    "\n",
    "        self.RNNWrapper = RNNWrapper(CustomSimpleRNNCell(units), return_sequences=False)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        \n",
    "        self.out = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    #@tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, data, training=False):\n",
    "        \n",
    "        x = self.RNNWrapper(data, training)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbe2d4",
   "metadata": {},
   "source": [
    "Lastly, we instantiate and use an RNN model on some random input as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a8f9122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The randomly initialized RNN model outputs the class probabilities \n",
      "[[0.09028652 0.09195881 0.09046976 0.10634772 0.12041804 0.11259319\n",
      "  0.10579    0.08614301 0.09627467 0.0997183 ]\n",
      " [0.09764748 0.09690416 0.09158446 0.11423371 0.09512214 0.10587687\n",
      "  0.09874821 0.08770555 0.10301013 0.10916731]]\n",
      " for the random input.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    input_sequence = tf.random.uniform(shape = (2,24,16), dtype=tf.float32)\n",
    "\n",
    "    hidden_state_size = 3\n",
    "\n",
    "    rnn = RNN_Model(hidden_state_size)\n",
    "    prediction = rnn(input_sequence)\n",
    "    tf.print(f\"The randomly initialized RNN model outputs the class probabilities \\n{prediction}\\n for the random input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d668e87",
   "metadata": {},
   "source": [
    "As a last thing, we can verify our RNN can take inputs of different lengths - one of the major advantages of using RNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13be2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous input shape was: (2, 24, 16)\n",
      "\n",
      "\n",
      "New input shape is: (8, 64, 16)\n",
      "\n",
      "\n",
      "We see that both the batch size and the sequence length are different and the RNN still works.\n",
      "This would not have worked with an MLP model that has different weights for each temporal part of the inputsequence. \n",
      "\n",
      "\n",
      "We obtain predictions of shape (8, 10) since we use a classification head with 10 units.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    \n",
    "    print(f\"Previous input shape was: {input_sequence.shape}\\n\\n\")\n",
    "    \n",
    "    input_sequence = tf.random.uniform(shape = (8,64,16), dtype=tf.float32)\n",
    "    \n",
    "    print(f\"New input shape is: {input_sequence.shape}\\n\\n\")\n",
    "    \n",
    "    print(\"We see that both the batch size and the sequence length are different and the RNN still works.\\n\\\n",
    "This would not have worked with an MLP model that has different weights for each temporal part of the input\\\n",
    "sequence. \\n\\n\")\n",
    "\n",
    "    # we re-use the same rnn model instantiation.\n",
    "    prediction = rnn(input_sequence)\n",
    "    tf.print(f\"We obtain predictions of shape {prediction.shape} since we use a classification head with 10 units.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d41ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn__model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rnn_wrapper (RNNWrapper)    multiple                  60        \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  512       \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,862\n",
      "Trainable params: 1,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ad18c",
   "metadata": {},
   "source": [
    "## **(Optional. This is not allowed for the homework)** The same model can also be defined with pre-defined layers and using the Functional API, which shows a nicer summary including output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df1efbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0749586448 0.0989045724 0.11350774 ... 0.0843760297 0.093167223 0.108546838]]\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None, 16)]        0         \n",
      "                                                                 \n",
      " rnn_6 (RNN)                 (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               512       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,862\n",
      "Trainable params: 1,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "\n",
    "    # a keras input which will be used to track computations done by the layers.\n",
    "    RNN_pseudo_input = tf.keras.Input(shape=(None,16), batch_size=None)\n",
    "    \n",
    "    # RNN feature extractor of the model using the layers we've defined\n",
    "    x = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(3), return_sequences=False)(RNN_pseudo_input)\n",
    "        \n",
    "    # classification head of the model\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x_out = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    \n",
    "    # instantiate model with functional api, tracing computations and layers between RNN_input and x_out\n",
    "    functional_model = tf.keras.Model(RNN_pseudo_input, x_out)\n",
    "\n",
    "    # input shape\n",
    "    input_sequence = tf.random.uniform(shape = (1,24,16), dtype=tf.float32)\n",
    "    \n",
    "    # call the functional model on the input sequence from before\n",
    "    prediction = functional_model(input_sequence)\n",
    "    \n",
    "    tf.print(prediction)\n",
    "    \n",
    "    functional_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni] *",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
