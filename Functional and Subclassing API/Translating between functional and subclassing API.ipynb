{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7116db",
   "metadata": {},
   "source": [
    "# Translating between and combining the functional and the subclassing API\n",
    "\n",
    "A lot of code that you will find online uses the functional API from tensorflow instead of the subclassing API. While translating the code between the APIs might seem difficult at first, it is rather straight forward. So before showing the architectural building blocks of the DenseNet, let's have a brief look at how to translate a simple model from the functional API to the subclassing API.\n",
    "\n",
    "While we do not allow the use of the functional API in the homeworks, it may come in handy when working on your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14bccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae5eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                544       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,306\n",
      "Trainable params: 3,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def composite_layer(x, dropout_rate):\n",
    "    # instantiate a dense layer and directly call it on the input\n",
    "    x = tf.keras.layers.Dense(32)(x)  \n",
    "    x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "    \n",
    "    # add dropout\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # do the same thing again\n",
    "    x = tf.keras.layers.Dense(64)(x)\n",
    "    x = tf.keras.layers.Activation(tf.nn.relu)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_dense_model(dropout_rate):\n",
    "    # we need to specify the input shape (ignoring the batch dimension)\n",
    "    x_in = tf.keras.Input((16))\n",
    "    \n",
    "    # call the composite layer function on the input\n",
    "    x = composite_layer(x_in, dropout_rate)\n",
    "    \n",
    "    # add an output layer\n",
    "    x_out = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=x_in, outputs=x_out)\n",
    "\n",
    "functional_model = get_dense_model(dropout_rate=0.5)\n",
    "\n",
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1108e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dense_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " composite_layer (CompositeL  multiple                 2656      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,306\n",
      "Trainable params: 3,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CompositeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(CompositeLayer, self).__init__()\n",
    "        \n",
    "        # instantiate layers in a list (useful if layers can be applied sequentially).\n",
    "        self.layer_list = [\n",
    "            \n",
    "            tf.keras.layers.Dense(32),\n",
    "            tf.keras.layers.Activation(tf.nn.relu),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(64),\n",
    "            tf.keras.layers.Activation(tf.nn.relu)\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        # apply the layers from the list sequentially\n",
    "        for layer in self.layer_list:\n",
    "            \n",
    "            # try to pass the training argument\n",
    "            try:\n",
    "                x = layer(x, training)\n",
    "                \n",
    "            # unless this does not work, then don't pass it\n",
    "            except:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class DenseModel(tf.keras.Model):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(DenseModel, self).__init__()\n",
    "        \n",
    "        self.composite_layer = CompositeLayer(dropout_rate)\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        x = self.composite_layer(x, training)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "subclassed_model = DenseModel(dropout_rate=0.5)\n",
    "\n",
    "# call the model on input to build the layers in it\n",
    "\n",
    "subclassed_model(tf.ones((1,16)));\n",
    "\n",
    "subclassed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141eb3c",
   "metadata": {},
   "source": [
    "Finally, we can mix and match between the two APIs (same for the sequential API which we do not cover here).\n",
    "\n",
    "So it is possible to write subclassed layers or modules and then use them within a functional API model. It is also possible to use functional API models as parts of a subclassed model or layer. The only difference that you will notice is that your layers will be encapsulated in models when you call model.layers or model.summary().\n",
    "\n",
    "Something to keep in mind is that functional models are created for specific input dimensions because they build the layers while instantiating the model. Subclassed models on the other hand need to be built by calling them on an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f4251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mixed_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_5 (Functional)        (None, 10)                3306      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,306\n",
      "Trainable params: 3,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# a functional API model that uses a subclassed layer\n",
    "\n",
    "x_in = tf.keras.Input((16))\n",
    "x = CompositeLayer(dropout_rate=0.5)(x_in)\n",
    "x_out = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mixed_model_a = tf.keras.Model(x_in, x_out)\n",
    "\n",
    "\n",
    "# a subclassed model that uses a functional API model as part of it\n",
    "\n",
    "class MixedModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MixedModel, self).__init__()\n",
    "        \n",
    "        self.functional_model = get_dense_model(dropout_rate=0.5)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        x = self.functional_model(x, training)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "mixed_model_b = MixedModel()\n",
    "\n",
    "mixed_model_b(tf.ones((1,16)))\n",
    "mixed_model_b.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni] *",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
