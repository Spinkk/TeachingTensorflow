{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0daa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import math\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f2dc2",
   "metadata": {},
   "source": [
    "# Using TensorBoard to track metrics\n",
    "\n",
    "In this notebook I will show how to use the TensorBoard extension which allows to keep track of training runs and data associated with each epoch (e.g. loss, accuracies, predicted/generated outputs, activation statistics etc.)\n",
    "\n",
    "First we will look at tensorflow's summary writer and what it can be used for before we use it in the training loop to keep track of the metrics.\n",
    "\n",
    "The summary writer can be used to log scalars, feature maps (e.g. batches of generated images or activations), histograms, audio (batches of 1D sequences), and text.\n",
    "\n",
    "\n",
    "After having taken a look at these, we will take a look at how this can be used to track the metrics (loss, accuracy etc.) of a subclassed model. For this we will implement the train and test step as internal methods of the model and have the loss, optimizer and metrics as attributes of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53f7ab",
   "metadata": {},
   "source": [
    "Here we iterate over 100 steps and try to store a loss, some randomly generated images, an audio tensor, text, and a histogram for each step. We do not yet use a deep learning model, we just show how to store different kinds of data with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1b8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# define file-path for log file\n",
    "file_path = \"test_logs/test\"\n",
    "\n",
    "# define the tf file-writer (we usually use a separate one for train and validation)\n",
    "summary_writer = tf.summary.create_file_writer(file_path)\n",
    "\n",
    "# write 100 logs for loss\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # compute loss (here targets and predictions would come from the data and the model)\n",
    "    targets = tf.constant([0.3,0.3,-0.8])\n",
    "    predictions = targets + tf.random.normal(shape=targets.shape, stddev=100/(i+1)) # decreasing noise\n",
    "    \n",
    "    loss_function = tf.keras.losses.MeanSquaredError()\n",
    "    loss = loss_function(targets,predictions)\n",
    "    \n",
    "    \n",
    "    # image batch (these would be obtained from the model)\n",
    "    \n",
    "    image_batch = tf.random.uniform(shape=(32,28,28,1),dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # audio batch (would be obtained from the model but here it's just a hard coded sine wave of 110hz)\n",
    "    \n",
    "    x = 2* math.pi*tf.cast(tf.linspace(0,32000*5, 32000*5), tf.float32)*110/32000\n",
    "    x = tf.expand_dims(x, axis=0) # add batch dimension\n",
    "    x = tf.expand_dims(x, axis=-1) # add last dimension\n",
    "    x = tf.repeat(x, 32, axis=0) # repeat to have a batch of 32\n",
    "    audio_batch = tf.math.sin(x) # obtain sine wave\n",
    "    \n",
    "    \n",
    "    # text (this would be the output of a language model after one training epoch)\n",
    "    \n",
    "    text_batch = tf.constant(\"This is the sampled output of a language model\")\n",
    "    \n",
    "    \n",
    "    # histogram (e.g. of activations of a dense layer during training)\n",
    "    \n",
    "    activations_batch = tf.random.normal(shape=(32,20,1))\n",
    "    min_activations = tf.reduce_min(activations_batch, axis=None)\n",
    "    max_activations = tf.reduce_max(activations_batch, axis=None)\n",
    "    histogram = tf.histogram_fixed_width_bins(activations_batch, \n",
    "                                              value_range=[min_activations, max_activations])\n",
    "    \n",
    "    \n",
    "    # now we want to write all the data to a log-file.\n",
    "    with summary_writer.as_default():\n",
    "        \n",
    "        # save the loss scalar for the \"epoch\"\n",
    "        tf.summary.scalar(name=\"loss\", data=loss, step=i)\n",
    "        \n",
    "        # save a batch of images for this epoch (have to be between 0 and 1)\n",
    "        tf.summary.image(name=\"generated_images\",data = image_batch, step=i, max_outputs=32)\n",
    "        \n",
    "        # save the batch of audio for this epoch\n",
    "        tf.summary.audio(name=\"generated_audio\", data = audio_batch, \n",
    "                         sample_rate = 32000, step=i, max_outputs=32)\n",
    "        \n",
    "        # save the generated text for that epoch\n",
    "        tf.summary.text(name=\"generated_text\", data = text_batch, step=i)\n",
    "        \n",
    "        # save a histogram (e.g. of activations in a layer)\n",
    "        tf.summary.histogram(name=\"layer_N_activations\", data = histogram, step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e422c5",
   "metadata": {},
   "source": [
    "# Inspect the logged data in the TensorBoard\n",
    "\n",
    "We can look at the images, audio, text, histograms and plots for each time-step. \n",
    "\n",
    "For plots under the \"scalars\" section, we can control the amount of smoothing for the plots. This allows us to visually judge whether the loss is decreasing even in the presence of strong oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8568d249",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11605), started 3:17:40 ago. (Use '!kill 11605' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13392e40749a50d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13392e40749a50d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open the tensorboard to inspect the data for the 100 steps\n",
    "%tensorboard --logdir test_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752cc5c",
   "metadata": {},
   "source": [
    "# Using TensorBoard to store loss and accuracy of a subclassed model\n",
    "\n",
    "In this part of the notebook, we will define a subclassed CNN model and store loss and accuracy for both training and validation data to the TensorBoard. \n",
    "\n",
    "To do this in a clean way, we implement the keras metrics that keep track of loss and accuracy in each epoch for us as part of the model. We also define the train and test steps as methods inside the model rather than as external functions. Doing so will move us one step closer to being able to use the in-built training and evaluation methods that come with Tensorflow/Keras, that is the compile and fit methods, which we do not yet allow for the homeworks.\n",
    "\n",
    "To use train_step and test_step as methods of the model, we need to have the loss-function, the metrics, and the optimizer as parts of the model, which is why we define them in the init method.\n",
    "\n",
    "Note that we need to update the metrics after each training example and reset the metrics after each epoch or before evaluating our model on the validation data set.\n",
    "\n",
    "Also note that the metrics_list contains a mean metric for the loss, which does not take targets and predictions as arguments in its update_state method, but just a scalar. For this reason, we treat it differently from the remaining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ff255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.metrics_list = [\n",
    "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "                        tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
    "                       ]\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)   \n",
    "        \n",
    "        L2_lambda = 0.01\n",
    "        dropout_amount = 0.5\n",
    "        \n",
    "        self.all_layers = [\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=32, \n",
    "                                   kernel_size=5, \n",
    "                                   strides=1, \n",
    "                                   padding=\"same\",\n",
    "                                   kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "                                   activation=None,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L2(L2_lambda)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.nn.relu),\n",
    "            \n",
    "            tf.keras.layers.MaxPool2D(pool_size=2,strides=1),\n",
    "            \n",
    "            tf.keras.layers.Dropout(dropout_amount),\n",
    "            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\",activation=None,\n",
    "                                  kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L2(L2_lambda)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(tf.nn.relu),\n",
    "            \n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_amount),\n",
    "\n",
    "            tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.L2(L2_lambda)),\n",
    "            tf.keras.layers.Activation(tf.nn.relu),\n",
    "            \n",
    "            tf.keras.layers.Dropout(dropout_amount),\n",
    "            \n",
    "            tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.L2(L2_lambda)),\n",
    "        ]\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "\n",
    "        for layer in self.all_layers:\n",
    "            try:\n",
    "                x = layer(x,training)\n",
    "            except:\n",
    "                x = layer(x)\n",
    "       \n",
    "        return x\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        x, targets = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(x, training=True)\n",
    "            \n",
    "            loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets,predictions)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "\n",
    "        x, targets = data\n",
    "        \n",
    "        predictions = self(x, training=False)\n",
    "        \n",
    "        loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "        \n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets, predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24901b2a",
   "metadata": {},
   "source": [
    "# Preparing the training and validation data\n",
    "\n",
    "Here we use data augmentation for which we use a special model that runs some operations on it (random flipping, resizing and cropping). Note that any kind of model can be used in the input pipeline - even a VAE could be used to encode the images and then add noise to the generative embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7af59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds =tfds.load(\"fashion_mnist\", as_supervised=True)\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"test\"]\n",
    "\n",
    "data_augmentation_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.Resizing(32,32),\n",
    "    tf.keras.layers.RandomCrop(28,28)\n",
    "])\n",
    "\n",
    "def augment(x):\n",
    "    return data_augmentation_model(x)\n",
    "\n",
    "train_ds = train_ds.map(lambda x,y: (augment(x)/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE).shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y: (x/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE).shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5068b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = CNN()\n",
    "\n",
    "# run model on input once so the layers are built\n",
    "model(tf.keras.Input((28,28,1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c372dcf",
   "metadata": {},
   "source": [
    "# Instantiate the file-writers for the training\n",
    "\n",
    "We store the tensorboard logs to a folder with a meaningful name (e.g. hyperparameter settings + date and time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f8a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clear all logs use this line:\n",
    "\n",
    "#!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474d53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "hyperparameter_string= \"Your_Settings_Here\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{hyperparameter_string}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{hyperparameter_string}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f322345",
   "metadata": {},
   "source": [
    "# Writing the training loop\n",
    "\n",
    "Note that you need to re-run the above cell (and hence update the time-stamp) if you don't want to over-write the data of the previous training-run.\n",
    "\n",
    "If you use keras metrics, do not forget to reset the states between train and validation and between epochs.\n",
    "We use metric.update_states(...) to update a metric. This usually means we update the running average with the new value. There also exist keras metrics that can also compute scores such as CategoricalAccuracy, TopKCategoricalAccuracy.\n",
    "\n",
    "For your own training loops, you may want to add TQDM to see the progress of each epoch and the estimate of how much time it will take.\n",
    "\n",
    "Instead of looking at the printed losses and accuracies, we can look at the TensorBoard plots which will be updated after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bdf983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "['loss: 1.609331727027893', 'acc: 0.5475833415985107', 'top-3-acc: 0.8635333180427551']\n",
      "['val_loss: 1.2642236948013306', 'val_acc: 0.6227999925613403', 'val_top-3-acc: 0.8938000202178955']\n",
      "\n",
      "\n",
      "Epoch 1:\n",
      "['loss: 1.1726185083389282', 'acc: 0.6559500098228455', 'top-3-acc: 0.9244666695594788']\n",
      "['val_loss: 1.4290522336959839', 'val_acc: 0.51419997215271', 'val_top-3-acc: 0.8436999917030334']\n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "['loss: 1.1076161861419678', 'acc: 0.6722833514213562', 'top-3-acc: 0.9319666624069214']\n",
      "['val_loss: 1.187070369720459', 'val_acc: 0.6211000084877014', 'val_top-3-acc: 0.9394999742507935']\n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "['loss: 1.0730130672454834', 'acc: 0.6803666949272156', 'top-3-acc: 0.936033308506012']\n",
      "['val_loss: 1.6239241361618042', 'val_acc: 0.5058000087738037', 'val_top-3-acc: 0.8766000270843506']\n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "['loss: 1.049936294555664', 'acc: 0.6890000104904175', 'top-3-acc: 0.9362499713897705']\n",
      "['val_loss: 1.0211036205291748', 'val_acc: 0.6879000067710876', 'val_top-3-acc: 0.9570000171661377']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    \n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    \n",
    "    # Training:\n",
    "    \n",
    "    for data in train_ds:\n",
    "        metrics = model.train_step(data)\n",
    "    \n",
    "    # print the metrics\n",
    "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
    "    \n",
    "    # logging the validation metrics to the log file which is used by tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "        for metric in model.metrics:\n",
    "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "    \n",
    "    # reset all metrics (requires a reset_metrics method in the model)\n",
    "    model.reset_metrics()\n",
    "    \n",
    "    \n",
    "    # Validation:\n",
    "    \n",
    "    for data in val_ds:\n",
    "        metrics = model.test_step(data)\n",
    "    \n",
    "    print([f\"val_{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
    "    \n",
    "    # logging the validation metrics to the log file which is used by tensorboard\n",
    "    with val_summary_writer.as_default():\n",
    "        for metric in model.metrics:\n",
    "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "    \n",
    "    # reset all metrics\n",
    "    model.reset_metrics()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ab2fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-744c2a47baf2f177\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-744c2a47baf2f177\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18965264",
   "metadata": {},
   "source": [
    "# Saving and loading a subclassed model\n",
    "\n",
    "Because training deep neural networks can take multiple days, weeks or even months, we want to save checkpoints in between. This is especially useful if you use Google Colab and you save the model directly to your Google Drive folder. That way you don't lose any progress if your runtime gets closed.\n",
    "\n",
    "Note however that you lose the state of the optimizer. I will provide another notebook that shows how a full model can be saved and loaded by using the in-built compile method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e9806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with a meaningful name\n",
    "model.save_weights(f\"saved_model_{hyperparameter_string}\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5aaca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new model from our CNN class\n",
    "loaded_model = CNN()\n",
    "\n",
    "# build the model\n",
    "inp= tf.keras.Input((28,28,1))\n",
    "loaded_model(inp)\n",
    "\n",
    "# load the model weights to continue training. \n",
    "loaded_model.load_weights(f\"saved_model_{hyperparameter_string}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni] *",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
