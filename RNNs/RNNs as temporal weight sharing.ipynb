{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c7cbfa",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Spinkk/TeachingTensorflow/blob/main/RNNs/RNNs%20as%20temporal%20weight%20sharing.ipynb)\n",
    "\n",
    "# Basic RNNs as fully connected network architectures with temporally shared weights\n",
    "\n",
    "In this notebook, we show how a basic RNN that uses a simple recurrent cell (no LSTM or GRU) transforms input by applying the same matrix multiplications at each time step, using the latest output as one of the inputs.\n",
    "\n",
    "Remember that tf.keras.layers.Dense layers perform a matrix multiplication on their input, so we use these layers to define our RNN cell.\n",
    "\n",
    "A basic RNN takes its own current hidden state (which is a vector of dimension n_outputs for each example in the batch) and linearly transforms it with a weight matrix, creating a vector of n_outputs dimensions. In addition (quite literally) to that, the RNN cell takes the input at the current time-step t (which is the feature vector in the example for time-step t). Adding the results from the two matrix multiplications together with a bias (and applying a tanh activation to the sum), we obtain the hidden state of the RNN-cell that will then be used at the next time-step t+1, together with the input data feature vector at t+1.\n",
    "\n",
    "More formally written, the computation of a simple RNN cell at one time-step can be described as follows:\n",
    "\n",
    "### $$h^{<t+1>} = tanh( W_{hh} h^{<t>} + W_{xh} x^{<t>} + b )$$ \n",
    "\n",
    "Where $x^{<t>}$ is the feature input at time-step t, and $W_{xh}$ is a matrix of shape x (the dimensionality of the input feature vector at a single time-step) by h (the dimensionality of the hidden state).\n",
    "\n",
    "\n",
    "Below we implement this type of an RNN on some input (not inside a tf.keras.Model or tf.keras.Layer but as separate layers since we don't do any model fitting here). First we show how to use tensorflow/keras inbuilt RNN cell and RNN wrapper layers to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70ee7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f192659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the RNN, which is the last hidden state is \n",
      "[[0.8912661  0.67020696 0.77448636]\n",
      " [0.6446345  0.4354966  0.7005535 ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    \n",
    "    # 1 batch with 24 time steps of 16 features. The time-dimension is the second after the batch dimension.\n",
    "    input_shape = (2, 24, 16)\n",
    "\n",
    "    # The length of the resulting vector (similar to the units argument in Dense layers)\n",
    "    n_outputs = 3\n",
    "    \n",
    "    input_sequence = tf.random.uniform(shape = input_shape)\n",
    "    \n",
    "    simple_RNN_cell = tf.keras.layers.SimpleRNNCell(n_outputs)\n",
    "    \n",
    "    # return_sequences=False means we only output the final hidden_state.\n",
    "    RNN = tf.keras.layers.RNN(simple_RNN_cell, return_sequences=False)\n",
    "    \n",
    "    output = RNN(input_sequence)\n",
    "    \n",
    "    print(f\"The output of the RNN, which is the last hidden state is \\n{output}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e4d2c",
   "metadata": {},
   "source": [
    "# tf.keras.layers.RNN and tf.keras.layers.SimpleRNNCell from scratch\n",
    "\n",
    "Now we want to look at how we can implement what happens inside the two pre-defined layers that we have used above.\n",
    "\n",
    "To do so, we create two dense layers carrying the matrices $W_{hh}$ and $W_{xh}$ that are involved in the simple RNN cell. For the bias $b$ we create a separate tf.variable. Notice that we disable the use of a bias in the dense layers and do not use an activation either, because we want a simple matrix multiplication.\n",
    "\n",
    "With the weight matrices and the bias defined, we can start to iterate over the same input sequence data that we have used before. The initial hidden state of the RNN cell is set to be a vector of zeros before looping over the input sequence.\n",
    "\n",
    "At each time-step we use the previous hidden-state of the cell and update it, following the equation shown in the beginning of the notebook. The output of this custom RNN is then just the final hidden-state vector.\n",
    "\n",
    "Notice how we use the same two weight matrices and the same bias for different temporal parts of the input data. This is why RNNs can be thought of as another variant of weight sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b57de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[-0.70980406  0.8229091   0.9880202 ]\n",
      " [ 0.01065047  0.912312    0.9980492 ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    ### We've seen the output of the randomly initialized simple RNN.\n",
    "    ### Now we want to show the computations that were involved by doing the same with dense layers \n",
    "    \n",
    "    ### Remember that dense layers without activation are just matrix multiplication of a weight matrix with input\n",
    "    \n",
    "    # We create the Dense layers (that will store the matrices)\n",
    "    dense_layer_hstate = tf.keras.layers.Dense(n_outputs, activation=None, use_bias=False)\n",
    "    dense_layer_input = tf.keras.layers.Dense(n_outputs, activation=None, use_bias=False)\n",
    "    bias = tf.Variable([0. for _ in range(n_outputs)])\n",
    "    \n",
    "    # create the initial hidden state\n",
    "    state = tf.zeros((1,n_outputs), tf.float32)\n",
    "    \n",
    "    # iterate over the time-steps\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        # on this iteration we use the input at timestep t\n",
    "        input_t =input_sequence[:,t,:]\n",
    "        \n",
    "        # we compute the sum of the input at t matrix multiplied, with the previous state matrix multiplied\n",
    "        # and an additional bias added.\n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "        \n",
    "        # finally we use tanh as an activation function to update the RNN cell state\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{state}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d88524",
   "metadata": {},
   "source": [
    "# Copying the weights of the pre-defined RNN to our custom RNN to verify the implementation\n",
    "\n",
    "To make sure that we do the same computations as in the pre-defined tf.keras.layers.SimpleRNNCell and the wrapper layer (which implements the for loop) tf.keras.layers.RNN, we take the weights that we have used before and assign them to the two dense layers and the bias. We observe that we now get the exact same output - our implementation is correct.\n",
    "\n",
    "This type of testing can be helpful to verify your custom implementations of the computations involved in the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cee053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the same weights as the pre-defined RNN, our custom RNN's output is\n",
      " [[0.8912661  0.67020696 0.77448636]\n",
      " [0.6446345  0.4354966  0.7005535 ]]\n",
      "\n",
      "The outputs of the pre-defined RNN and our custom RNN are the same: True\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    # Now we want to copy the weights of this RNN to a fully connected model to obtain the same output\n",
    "    \n",
    "    RNN_cell_input_weights = RNN.trainable_variables[0]\n",
    "\n",
    "    RNN_cell_hstate_weights = RNN.trainable_variables[1]\n",
    "    \n",
    "    RNN_cell_biases = RNN.trainable_variables[2]\n",
    "    \n",
    "    dense_layer_hstate.weights[0].assign(tf.reshape(RNN_cell_hstate_weights, dense_layer_hstate.weights[0].shape))\n",
    "    dense_layer_input.weights[0].assign(tf.reshape(RNN_cell_input_weights, dense_layer_input.weights[0].shape))\n",
    "    bias.assign(RNN_cell_biases)\n",
    "    \n",
    "    # again we run our custom RNN, this time with the same weights as the tf.keras.layers.RNN version\n",
    "    \n",
    "    # same code as above\n",
    "    state = tf.zeros((1,n_outputs), tf.float32)\n",
    "\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        input_t =input_sequence[:,t,:]\n",
    "        \n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "    \n",
    "    print(f\"With the same weights as the pre-defined RNN, our custom RNN's output is\\n {state}\\n\")\n",
    "    \n",
    "    print(f\"The outputs of the pre-defined RNN and our custom RNN are the same: {tf.reduce_all(state==output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7910b2",
   "metadata": {},
   "source": [
    "# A note on using RNNs to return a sequence of hidden-states\n",
    "\n",
    "Before we set the return_sequences argument to False and we ended up with a single vector as the output of the RNN (the final hidden state). To obtain a sequence from the RNN, we want to return the hidden states from each time-step. This is useful for cases in which we want to translate from one sequence to another as in language translation. To implement this with the pre-defined layers, we use the same cell layer as before but re-instantiate the RNN wrapper layer with the new argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b005af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the RNN, which is the last hidden state is \n",
      " [[[ 0.3396979   0.6822263   0.5196427 ]\n",
      "  [ 0.17712753 -0.30245814  0.40522677]\n",
      "  [ 0.87390345  0.85211354  0.7118771 ]\n",
      "  [ 0.65884954 -0.38833636  0.7659966 ]\n",
      "  [ 0.96329224  0.7859971   0.6243708 ]\n",
      "  [ 0.515611   -0.76265275  0.6495711 ]\n",
      "  [ 0.95947945  0.8897012   0.7111566 ]\n",
      "  [ 0.5496499  -0.50042963  0.3595592 ]\n",
      "  [ 0.914206    0.78939044  0.8333135 ]\n",
      "  [ 0.81257606 -0.28883561  0.63706404]\n",
      "  [ 0.77599657  0.2869477   0.36363408]\n",
      "  [ 0.8101806   0.6166689   0.5593483 ]\n",
      "  [ 0.43481672 -0.36114278  0.23310821]\n",
      "  [ 0.555743    0.41584212  0.6592722 ]\n",
      "  [ 0.74648666  0.7087062   0.76838   ]\n",
      "  [ 0.39154133 -0.39427656  0.06196904]\n",
      "  [ 0.90339917  0.8030102   0.85790867]\n",
      "  [ 0.31084213 -0.60803187  0.3543192 ]\n",
      "  [ 0.73040915  0.7899899   0.7256308 ]\n",
      "  [ 0.6208359  -0.30283618  0.02103382]\n",
      "  [ 0.15764168  0.57924265  0.94759715]\n",
      "  [ 0.5922894   0.5579112   0.44225782]\n",
      "  [ 0.7010376  -0.04795119  0.12987894]\n",
      "  [ 0.8912661   0.67020696  0.77448636]]\n",
      "\n",
      " [[ 0.2238707   0.74148226  0.72954786]\n",
      "  [ 0.3499984   0.13402337 -0.26202235]\n",
      "  [ 0.77480966  0.93784106  0.6268909 ]\n",
      "  [ 0.66692907  0.12041342  0.36197963]\n",
      "  [ 0.63787955  0.50397575  0.7733231 ]\n",
      "  [ 0.65171945 -0.02899501  0.62384385]\n",
      "  [ 0.8809097   0.74423516  0.27330598]\n",
      "  [ 0.3206771   0.26228976  0.81799626]\n",
      "  [ 0.70060503  0.01640721  0.76144725]\n",
      "  [ 0.79903424  0.35640156  0.7533313 ]\n",
      "  [ 0.65446514  0.75386864  0.398477  ]\n",
      "  [ 0.31244507 -0.3817884   0.29792795]\n",
      "  [ 0.91403925  0.8873897   0.7989589 ]\n",
      "  [ 0.7966653   0.2991037  -0.18304002]\n",
      "  [ 0.5410115   0.2754744   0.7872791 ]\n",
      "  [ 0.759702    0.53247625  0.28737015]\n",
      "  [ 0.64108205 -0.19745626  0.7065711 ]\n",
      "  [ 0.69174635  0.20829353  0.83722955]\n",
      "  [ 0.93136114  0.22194618 -0.06379695]\n",
      "  [ 0.50603145  0.27870414  0.7946686 ]\n",
      "  [ 0.47467548 -0.21500793 -0.05157277]\n",
      "  [ 0.683763    0.5762002   0.730389  ]\n",
      "  [ 0.9430287   0.4034074   0.48316926]\n",
      "  [ 0.6446345   0.4354966   0.7005535 ]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "    # simple_RNN_cell and input_sequence remain unchanged\n",
    "    \n",
    "    # return_sequences=True means we output the hidden_states of all time-steps.\n",
    "    RNN = tf.keras.layers.RNN(simple_RNN_cell, return_sequences=True)\n",
    "    \n",
    "    RNN_outputs = RNN(input_sequence)\n",
    "    \n",
    "    print(f\"The output of the RNN, which is the last hidden state is \\n {RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ed762",
   "metadata": {},
   "source": [
    "# Return sequences from scratch\n",
    "\n",
    "To implement this in a way that will allow you to also use this in tensorflow models that use the @tf.function decorator, we can't just create a list of the hidden states and append to it - tensorflow does not allow list appends in graph mode. Instead we need to make use of a tf.TensorArray object (this also allows for the case in which we do not know how many time-steps we will process and thus can't know beforehand how many hidden-states we want to output).\n",
    "\n",
    "A tf.TensorArray object is not just a tensor and thus we need to call the .stack() method on it to obtain the hidden_states that it stores. Since the result has the batch-dimension and the time-dimension switched, we make use of a permuted transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c416bf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[[ 0.3396979   0.6822263   0.5196427 ]\n",
      "  [ 0.17712753 -0.30245814  0.40522677]\n",
      "  [ 0.87390345  0.85211354  0.7118771 ]\n",
      "  [ 0.65884954 -0.38833636  0.7659966 ]\n",
      "  [ 0.96329224  0.7859971   0.6243708 ]\n",
      "  [ 0.515611   -0.76265275  0.6495711 ]\n",
      "  [ 0.95947945  0.8897012   0.7111566 ]\n",
      "  [ 0.5496499  -0.50042963  0.3595592 ]\n",
      "  [ 0.914206    0.78939044  0.8333135 ]\n",
      "  [ 0.81257606 -0.28883561  0.63706404]\n",
      "  [ 0.77599657  0.2869477   0.36363408]\n",
      "  [ 0.8101806   0.6166689   0.5593483 ]\n",
      "  [ 0.43481672 -0.36114278  0.23310821]\n",
      "  [ 0.555743    0.41584212  0.6592722 ]\n",
      "  [ 0.74648666  0.7087062   0.76838   ]\n",
      "  [ 0.39154133 -0.39427656  0.06196904]\n",
      "  [ 0.90339917  0.8030102   0.85790867]\n",
      "  [ 0.31084213 -0.60803187  0.3543192 ]\n",
      "  [ 0.73040915  0.7899899   0.7256308 ]\n",
      "  [ 0.6208359  -0.30283618  0.02103382]\n",
      "  [ 0.15764168  0.57924265  0.94759715]\n",
      "  [ 0.5922894   0.5579112   0.44225782]\n",
      "  [ 0.7010376  -0.04795119  0.12987894]\n",
      "  [ 0.8912661   0.67020696  0.77448636]]\n",
      "\n",
      " [[ 0.2238707   0.74148226  0.72954786]\n",
      "  [ 0.3499984   0.13402337 -0.26202235]\n",
      "  [ 0.77480966  0.93784106  0.6268909 ]\n",
      "  [ 0.66692907  0.12041342  0.36197963]\n",
      "  [ 0.63787955  0.50397575  0.7733231 ]\n",
      "  [ 0.65171945 -0.02899501  0.62384385]\n",
      "  [ 0.8809097   0.74423516  0.27330598]\n",
      "  [ 0.3206771   0.26228976  0.81799626]\n",
      "  [ 0.70060503  0.01640721  0.76144725]\n",
      "  [ 0.79903424  0.35640156  0.7533313 ]\n",
      "  [ 0.65446514  0.75386864  0.398477  ]\n",
      "  [ 0.31244507 -0.3817884   0.29792795]\n",
      "  [ 0.91403925  0.8873897   0.7989589 ]\n",
      "  [ 0.7966653   0.2991037  -0.18304002]\n",
      "  [ 0.5410115   0.2754744   0.7872791 ]\n",
      "  [ 0.759702    0.53247625  0.28737015]\n",
      "  [ 0.64108205 -0.19745626  0.7065711 ]\n",
      "  [ 0.69174635  0.20829353  0.83722955]\n",
      "  [ 0.93136114  0.22194618 -0.06379695]\n",
      "  [ 0.50603145  0.27870414  0.7946686 ]\n",
      "  [ 0.47467548 -0.21500793 -0.05157277]\n",
      "  [ 0.683763    0.5762002   0.730389  ]\n",
      "  [ 0.9430287   0.4034074   0.48316926]\n",
      "  [ 0.6446345   0.4354966   0.7005535 ]]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:cpu:0'):\n",
    "\n",
    "    state = tf.zeros((input_sequence.shape[0],n_outputs), tf.float32)\n",
    "\n",
    "    # initialize the hidden_states TensorArray that we want to output (shape is: batch, time-steps, h_dim)\n",
    "    hidden_states = tf.TensorArray(dtype=tf.float32, size = input_sequence.shape[1])\n",
    "\n",
    "    for t in tf.range(input_shape[1]):\n",
    "        input_t =input_sequence[:,t,:]\n",
    "\n",
    "        x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "        state = tf.nn.tanh(x_sum)\n",
    "\n",
    "        # write the states to the TensorArray\n",
    "        hidden_states = hidden_states.write(t, state)\n",
    "\n",
    "    # transpose the sequence of hidden_states from TensorArray accordingly (batch and time dimensions switched)\n",
    "    custom_RNN_outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
    "\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{custom_RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186ca1",
   "metadata": {},
   "source": [
    "Now let us verify that the @tf.function decorator works with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb25148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the custom RNN that we built with dense layers is: \n",
      "[[[ 0.3396979   0.6822263   0.5196427 ]\n",
      "  [ 0.17712753 -0.30245814  0.40522677]\n",
      "  [ 0.87390345  0.85211354  0.7118771 ]\n",
      "  [ 0.65884954 -0.38833636  0.7659966 ]\n",
      "  [ 0.96329224  0.7859971   0.6243708 ]\n",
      "  [ 0.515611   -0.76265275  0.6495711 ]\n",
      "  [ 0.95947945  0.8897012   0.7111566 ]\n",
      "  [ 0.5496499  -0.50042963  0.3595592 ]\n",
      "  [ 0.914206    0.78939044  0.8333135 ]\n",
      "  [ 0.81257606 -0.28883561  0.63706404]\n",
      "  [ 0.77599657  0.2869477   0.36363408]\n",
      "  [ 0.8101806   0.6166689   0.5593483 ]\n",
      "  [ 0.43481672 -0.36114278  0.23310821]\n",
      "  [ 0.555743    0.41584212  0.6592722 ]\n",
      "  [ 0.74648666  0.7087062   0.76838   ]\n",
      "  [ 0.39154133 -0.39427656  0.06196904]\n",
      "  [ 0.90339917  0.8030102   0.85790867]\n",
      "  [ 0.31084213 -0.60803187  0.3543192 ]\n",
      "  [ 0.73040915  0.7899899   0.7256308 ]\n",
      "  [ 0.6208359  -0.30283618  0.02103382]\n",
      "  [ 0.15764168  0.57924265  0.94759715]\n",
      "  [ 0.5922894   0.5579112   0.44225782]\n",
      "  [ 0.7010376  -0.04795119  0.12987894]\n",
      "  [ 0.8912661   0.67020696  0.77448636]]\n",
      "\n",
      " [[ 0.2238707   0.74148226  0.72954786]\n",
      "  [ 0.3499984   0.13402337 -0.26202235]\n",
      "  [ 0.77480966  0.93784106  0.6268909 ]\n",
      "  [ 0.66692907  0.12041342  0.36197963]\n",
      "  [ 0.63787955  0.50397575  0.7733231 ]\n",
      "  [ 0.65171945 -0.02899501  0.62384385]\n",
      "  [ 0.8809097   0.74423516  0.27330598]\n",
      "  [ 0.3206771   0.26228976  0.81799626]\n",
      "  [ 0.70060503  0.01640721  0.76144725]\n",
      "  [ 0.79903424  0.35640156  0.7533313 ]\n",
      "  [ 0.65446514  0.75386864  0.398477  ]\n",
      "  [ 0.31244507 -0.3817884   0.29792795]\n",
      "  [ 0.91403925  0.8873897   0.7989589 ]\n",
      "  [ 0.7966653   0.2991037  -0.18304002]\n",
      "  [ 0.5410115   0.2754744   0.7872791 ]\n",
      "  [ 0.759702    0.53247625  0.28737015]\n",
      "  [ 0.64108205 -0.19745626  0.7065711 ]\n",
      "  [ 0.69174635  0.20829353  0.83722955]\n",
      "  [ 0.93136114  0.22194618 -0.06379695]\n",
      "  [ 0.50603145  0.27870414  0.7946686 ]\n",
      "  [ 0.47467548 -0.21500793 -0.05157277]\n",
      "  [ 0.683763    0.5762002   0.730389  ]\n",
      "  [ 0.9430287   0.4034074   0.48316926]\n",
      "  [ 0.6446345   0.4354966   0.7005535 ]]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 19:55:04.471725: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/device:cpu:0'):\n",
    "    @tf.function\n",
    "    def tf_func():\n",
    "\n",
    "        state = tf.zeros((input_sequence.shape[0],n_outputs), tf.float32)\n",
    "\n",
    "\n",
    "        # initialize the hidden_states TensorArray that we want to output (shape is: batch, time-steps, h_dim)\n",
    "        hidden_states = tf.TensorArray(dtype=tf.float32, size=input_sequence.shape[1])\n",
    "\n",
    "        for t in tf.range(input_shape[1]):\n",
    "            input_t =input_sequence[:,t,:]\n",
    "\n",
    "            x_sum = dense_layer_input(input_t) + dense_layer_hstate(state) + bias\n",
    "\n",
    "            state = tf.nn.tanh(x_sum)\n",
    "\n",
    "            # write the states to the TensorArray\n",
    "            hidden_states = hidden_states.write(t, state)\n",
    "\n",
    "        # transpose the sequence of hidden_states from TensorArray accordingly (batch and time dimensions switched)\n",
    "        custom_RNN_outputs = tf.transpose(hidden_states.stack(), [1,0,2])\n",
    "        \n",
    "        return custom_RNN_outputs\n",
    "    custom_RNN_outputs = tf_func()\n",
    "    print(f\"The output of the custom RNN that we built with dense layers is: \\n{custom_RNN_outputs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f87760",
   "metadata": {},
   "source": [
    "Finally, we again verify that the outputs with return_sequences=True match between our custom implementation of the computations involved and the pre-defined RNN-wrapper and cell. Since we did not re-initialize the weights, the outputs should still be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc99e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_all(custom_RNN_outputs == RNN_outputs).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni] *",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
